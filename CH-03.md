## Page 1
Instruction Level Parallelism (ILP) is a concept in computer architecture that involves the simultaneous execution of multiple instructions from a program. Unlike pipelining, which is a basic form of ILP, more aggressive techniques can be employed to achieve greater parallelism in instruction execution. Here's an illustration to demonstrate ILP:

Imagine you have a simple program consisting of the following instructions:

- Load data from memory
- Add two numbers
- Store the result back to memory
- Multiply two numbers
- Subtract one number from another

In a traditional, non-ILP execution model, each instruction would be executed sequentially, one after the other. However, with ILP, multiple instructions can be executed simultaneously, exploiting various forms of parallelism within the instruction stream.

Here's how ILP might be applied to execute these instructions simultaneously:

```sql
Clock Cycle: 1
Instruction 1: Load data from memory
Instruction 2: Add two numbers

Clock Cycle: 2
Instruction 3: Store the result back to memory
Instruction 4: Multiply two numbers

Clock Cycle: 3
Instruction 5: Subtract one number from another

Clock Cycle: 4
Idle cycle or next set of instructions (if available)
```

In this illustration, multiple instructions are executed concurrently in each clock cycle, maximizing the utilization of computational resources and improving overall performance. This is achieved by identifying and exploiting independent instructions that can be executed simultaneously without affecting the program's correctness.

---

## Page 2
Exploiting Instruction Level Parallelism (ILP) typically involves two fundamental approaches:

2. Hardware-Based ILP:

  - `Dynamic Parallelism Discovery and Exploitation`: In this approach, the hardware dynamically identifies and exploits parallelism within the instruction stream during runtime. Modern processors employ various techniques such as superscalar execution, out-of-order execution, and speculative execution to execute multiple instructions simultaneously. For example, a superscalar processor can issue multiple instructions from a single instruction stream to different execution units in parallel, while out-of-order execution allows instructions to be executed as soon as their operands are available, regardless of their original program order.
2. Software-Based ILP:

  - `Static Program Restructuring for Parallelism`: In this approach, software developers or compilers restructure programs to facilitate parallel execution statically, i.e., at compile time. This may involve techniques such as loop unrolling, function inlining, and software pipelining. By restructuring the program's code, the compiler aims to expose and exploit parallelism inherent in the program's structure. For example, loop unrolling duplicates loop iterations to allow more opportunities for parallel execution, while function inlining reduces overhead by replacing function calls with the actual function code.

These two approaches are complementary and can be combined to enhance performance further. Hardware-based ILP mechanisms leverage the dynamic capabilities of modern processors to exploit parallelism at runtime, while software-based ILP techniques aim to optimize the program's structure to better utilize hardware resources. By employing both approaches in concert, developers can achieve significant improvements in performance by maximizing the utilization of available parallelism at both hardware and software levels.

---

## Page 3

The challenges and opportunities related to Instruction Level Parallelism (ILP) in a program can be understood through various levels of code organization, such as basic blocks and loops. Let's illustrate these concepts:

1. `Basic Block`:
  - A basic block is a straight-line code sequence within a program with a single entry point and a single exit point. These are fundamental units of code execution where the execution flow enters at the beginning and exits at the end without any branching or jumping within the block.
One challenge with basic blocks is that they often contain a limited number of instructions between branch instructions. Since branches occur approximately 15%–25% of the time on average, there are typically only 3–6 instructions between a pair of branches.

  - However, basic blocks also present opportunities for ILP, particularly in executing the instructions within the block simultaneously. Hardware-based ILP mechanisms like superscalar execution can exploit parallelism within basic blocks by executing multiple instructions concurrently.

2. `Loop Level Parallelism`:

  - Loop level parallelism refers to the opportunity to use multiple iterations of a loop to expose additional parallelism. Loops are a common construct in programs, and optimizing them for parallel execution can significantly enhance performance.

  - Example techniques to exploit loop level parallelism include:
    - `Vectorization`: Converting scalar operations into vector operations, enabling multiple data elements to be processed simultaneously by vectorized instructions.
    - `Data Parallelism`: Distributing data across multiple processing units and executing the same operation on different data elements simultaneously.
    - `Loop Unrolling`: Duplicating loop iterations to increase the number of independent instructions available for parallel execution. This reduces loop overhead and exposes more parallelism.

- These techniques aim to exploit parallelism inherent in loops, allowing multiple iterations to execute concurrently or overlap in execution, thereby improving overall throughput and performance.

In summary, understanding the characteristics of basic blocks and loops in a program helps identify both challenges and opportunities for exploiting ILP. By leveraging hardware capabilities and employing optimization techniques at the basic block and loop levels, developers can maximize the utilization of parallel execution resources and enhance program performance.

---

## Page 4

Dependencies and hazards are crucial concepts in understanding the limitations and challenges of pipelined execution in computer architecture. Let's illustrate the three types of dependencies and how they can manifest as hazards in a pipeline:

1. `Data Dependencies (True Data Dependencies)`:
  - Data dependencies occur when the result of one instruction is needed as an input for another instruction.
  - Example:

```powershell
Instruction 1: ADD R1, R2, R3   ; R1 = R2 + R3
Instruction 2: SUB R4, R1, R5   ; R4 = R1 - R5
```
Instruction 2 depends on the result produced by Instruction 1. This is a data dependency.

2. `Name Dependencies (Anti-dependencies and Output Dependencies)`:
  - Name dependencies occur when two instructions use the same register or memory location, and one of them modifies the value before the other can use it.
  - Example (Output Dependency):
```powershell
Instruction 1: ADD R1, R2, R3   ; R1 = R2 + R3
Instruction 2: ADD R1, R1, R4   ; R1 = R1 + R4
```
Instruction 2 depends on the result produced by Instruction 1, but it also modifies the same register (R1), causing a name dependency.

3. `Control Dependencies`:
  - Control dependencies arise due to conditional branches in the code. Instructions following a branch may depend on the outcome of the branch.
  - Example:
```powershell
Instruction 1: CMP R1, R2         ; Compare R1 and R2
Instruction 2: JNE Label          ; Jump if not equal
Instruction 3: ADD R3, R4, R5     ; R3 = R4 + R5
```
Instruction 3 depends on the outcome of the conditional branch (Instruction 2).

Now, let's illustrate how these dependencies can turn into hazards within a pipeline:

- `Data Hazard`:
  - When an instruction depends on the result of a previous instruction, but the result is not yet available due to pipeline delays, it results in a data hazard.
  - For example, in a pipeline, if Instruction 2 needs the result produced by Instruction 1 but Instruction 1 is still in the execution stage, a data hazard occurs.
- `Name Hazard`:
  - When two instructions attempt to access the same register or memory location simultaneously, causing a conflict, it results in a name hazard.
  - For example, if Instruction 2 attempts to write to a register that Instruction 1 is also writing to, it creates a name hazard.
- `Control Hazard (Branch Hazard)`:
  - When the outcome of a branch instruction is not yet known, but subsequent instructions in the pipeline have already been fetched or partially executed based on the predicted outcome, it results in a control hazard.
  - For example, if Instruction 2 is a branch instruction and subsequent instructions are already fetched assuming the branch outcome, but the actual outcome is different, it creates a control hazard.

In summary, dependencies are inherent in the program's logic, while hazards arise due to the pipeline's organization and execution. Proper handling of dependencies and hazards is essential for ensuring correct and efficient pipeline execution.

---

## Page 5
Let's illustrate data dependency with a simple example:

Consider the following sequence of instructions:

1. `ADD R1, R2, R3` (Add the contents of registers R2 and R3 and store the result in R1)
2. `SUB R4, R1, R5` (Subtract the contents of register R5 from R1 and store the result in R4)
3. `MUL R6, R1, R7` (Multiply the contents of registers R1 and R7 and store the result in R6)

In this sequence, there are data dependencies between instructions:

1. Instruction 2 (`SUB`) is dependent on instruction 1 (`ADD`) because it uses the result produced by instruction 1 (value in register R1).
2. Instruction 3 (`MUL`) is also dependent on instruction 1 (`ADD`) because it uses the result produced by instruction 1 (value in register R1).

So, instruction 2 and instruction 3 are both data dependent on instruction 1.

This dependency arises because the result produced by instruction 1 (`ADD`) is required as an input for both instruction 2 (`SUB`) and instruction 3 (`MUL`). As a result, instructions 2 and 3 cannot execute until the result produced by instruction 1 is available, indicating a data dependency between these instructions.

---

## Page 6

Let's illustrate name dependencies, specifically antidependence and output dependence, with a simple example:

Consider the following sequence of instructions:

1. `LOAD R1, [A]` (Load the contents of memory location A into register R1)
2. `ADD R1, R2, R3` (Add the contents of registers R2 and R3 and store the result in R1)
3. `STORE R1, [B]` (Store the contents of register R1 into memory location B)

In this sequence, there are name dependencies between instructions:

1. `Antidependence`:
  - Instruction 3 (`STORE`) writes to the memory location that instruction 1 (`LOAD`) reads from. This is an antidependence because instruction 3 writes to memory location `[B]`, which instruction 1 reads from `[A]`. However, there is no data flow between these instructions; they merely share the same register (`R1`) as an intermediary.

  - If instruction 3 were to execute before instruction 1, it could potentially overwrite the value in register `R1` before it is loaded with the value from memory location `[A]`, causing incorrect behavior.

2. `Output Dependence`:
  - Instruction 2 (`ADD`) and instruction 3 (`STORE`) both write to register `R1`. This is an output dependence because both instructions write to the same register (`R1`), but there is no flow of data dependency between them.

  - If instruction 2 were to execute before instruction 3, it could potentially modify the value of register `R1`, affecting the result stored by instruction 3.

Since these are not true dependencies, instructions 1 and 3 could potentially be executed in parallel if these dependencies are resolved, for example, by using distinct registers or memory locations. By eliminating these dependencies, the instructions can execute concurrently without interfering with each other.

> NOTE: True dependency is a conventional name for Data dependency

---

## Page 7

Data hazards occur when there's a possibility of incorrect program execution due to dependencies between instructions. These hazards arise when instructions are executed in a pipeline, and their overlapping execution violates the order of dependencies required by the program. There are several types of data hazards:

1. `RAW Hazard (Read After Write)`:
  - Occurs when an instruction tries to read data before a prior instruction writes to the same data.
  - For example, if instruction A writes to a register and instruction B reads from the same register before the value is written by instruction A.

2. `WAW Hazard (Write After Write)`:
  - Occurs when two instructions try to write to the same data location in consecutive or overlapping clock cycles.
  - For example, if instruction A and instruction B both write to the same register or memory location without any intervening read operations.

3. `WAR Hazard (Write After Read)`:

  - Occurs when an instruction tries to write to a data location before a prior instruction reads from the same data location.
  - For example, if instruction A reads from a register or memory location and instruction B writes to the same register or memory location before the value is read by instruction A.

4. `RAR Hazard (Read After Read)`:

  - Unlike the other hazards, RAR is not considered a hazard because it does not result in incorrect program behavior.
  - Occurs when two instructions both try to read from the same data location.
  - Since reads don't affect the data being read, there's no hazard associated with this situation.

Data hazards need to be handled carefully in pipelined processors to ensure correct program execution. Techniques such as forwarding (also known as data bypassing) and stalling (also known as pipeline bubbling) are commonly used to mitigate data hazards and maintain the correct order of instructions in the pipeline.

---

## Page 8

Control dependencies refer to the dependency of instructions on the sequential flow of execution, particularly in relation to branch instructions (or any operation that alters the program flow). These dependencies ensure that the behavior of the program, especially regarding branches, is preserved. Let's illustrate control dependency with a simple example:

Consider the following code snippet:

```python
if condition:
    x = 1
else:
    x = 2
y = x + 1
```

In this code, the value of x depends on the condition (condition). If the condition is true, x is assigned the value 1; otherwise, it is assigned the value 2. Then, y is assigned the value of x plus 1.

Control dependencies come into play here:

1.`Instruction Control Dependency`:
  - The instruction `y = x + 1` is control dependent on the preceding branch instruction (`if` statement). The execution of `y = x + 1` depends on the outcome of the condition (`condition`).
  - The constraint imposed by control dependency ensures that `y = x + 1` cannot be moved before the branch instruction. Moving it before the branch would alter the program's behavior because `y` would be assigned a value without considering the condition's outcome.
  - Similarly, any instruction that follows the branch (like printing the value of `y`) is not control-dependent on the branch, meaning it cannot be moved inside the branch block.

2. `Branch Control Dependency`:
  - The branch instruction (`if` statement) controls the execution flow of subsequent instructions based on the condition. Any instruction following the branch is control-dependent on it.
  - The constraint imposed by control dependency ensures that instructions that are control-dependent on the branch cannot be moved before the branch. Moving such instructions would alter the program's behavior because their execution should be determined by the branch's outcome.

In summary, control dependencies ensure that instructions are executed in the correct order to maintain the intended behavior of the program, particularly concerning branches or other flow-altering operations. They dictate the sequential flow of execution and preserve the program's control structure.

---

## page 9

In simpler terms, strict adherence to dependency rules isn't always necessary as long as we maintain the correctness of the program. There are two critical properties that must be preserved:

1. `Exception Behavior`:
  - When reordering instructions, we must ensure that the program's exception behavior remains unchanged. This means that visible exceptions, such as errors or interrupts that affect program execution, should not be altered.
  - For example, if a program is designed to handle divide-by-zero errors by raising an exception, reordering instructions should not cause the program to ignore or mishandle such errors.

2. `Data Flow`:
  - The flow of data within the program must be preserved. This means that data sources must remain accessible to the instructions that need them.
  - For example, if a variable x is used in multiple parts of the program, reordering instructions should not result in x being modified or overwritten before all its uses are complete.

Additionally, there's the concept of liveness:

- **Liveness**: Data that is actively needed for computation is called "live," while data that is no longer used can be considered "dead."
- For example, if a variable y is used only at the beginning of the program but never again afterward, it becomes "dead" once its use is complete. Conversely, if y continues to be used throughout the program, it remains "live" until all its uses are finished.

In essence, while it's important to maintain dependency rules to ensure program correctness, there's some flexibility in how strictly we adhere to them, as long as we preserve critical aspects such as exception behavior, data flow, and liveness. This allows for optimization and performance improvements without sacrificing correctness.

---

## page 10

Compiler techniques play a crucial role in exposing Instruction Level Parallelism (ILP) to enhance program performance. Two common techniques used by compilers are loop unrolling and instruction scheduling. Let's illustrate each of these techniques:

1. `Loop Unrolling`:
  - Loop unrolling is a compiler optimization technique where the compiler replicates the body of a loop multiple times, effectively reducing the loop overhead and exposing more opportunities for parallelism.
  - Example:
```c
// Original loop
for (int i = 0; i < 4; i++) {
    array[i] = array[i] + 1;
}
```
After loop unrolling:
```c
// Unrolled loop
array[0] = array[0] + 1;
array[1] = array[1] + 1;
array[2] = array[2] + 1;
array[3] = array[3] + 1;
```
- By unrolling the loop, the compiler exposes multiple independent iterations that can potentially execute in parallel, thereby improving performance.

2. `Instruction Scheduling`:
  - Instruction scheduling rearranges the order of instructions in a program to maximize the utilization of execution units and minimize pipeline stalls.
  - Example:
```assembly
; Original code
ADD R1, R2, R3
SUB R4, R1, R5
```
After instruction scheduling:
```assembly
; Scheduled code
SUB R4, R1, R5
ADD R1, R2, R3
```
- In this example, the instruction scheduling reorders the instructions to ensure that the result produced by the first instruction (`ADD`) is available for use by the second instruction (`SUB`) without introducing pipeline stalls.

- By rearranging instructions intelligently, the compiler can expose more opportunities for parallel execution and reduce dependencies, thereby improving ILP and overall program performance.

In summary, loop unrolling and instruction scheduling are two important compiler techniques for exposing Instruction Level Parallelism. They optimize the program's structure and instruction ordering to maximize parallelism and minimize pipeline stalls, ultimately improving the efficiency and performance of the executed code.

---

## Page 11

Advanced branch prediction techniques aim to improve the accuracy of predicting the outcome of branch instructions in a program, thereby reducing the number of pipeline stalls and improving overall performance. Let's illustrate two advanced branch prediction techniques:

1. `Correlating Branch Predictors (Two-Level Predictors)`:
  - Correlating branch predictors, also known as two-level predictors, make use of the outcomes of recent branch instructions to make predictions.
  - These predictors maintain a history table that records the outcomes of recent branches. Based on this history, the predictor uses a pattern matching algorithm to predict the outcome of the current branch.
  - Example:
    - Let's consider a two-bit saturating counter as a simple correlating predictor. Each entry in the history table corresponds to a branch instruction and consists of a two-bit counter.
    - If the counter value is "00" or "01," it predicts "not taken," and if it's "10" or "11," it predicts "taken."
    - When a branch is encountered, the predictor consults the history table based on the branch's address and uses the corresponding counter value to predict the branch outcome.
    - After the branch's outcome is determined, the counter value is updated based on the actual outcome of the branch.
  - By correlating the outcomes of recent branches, these predictors can capture patterns in branch behavior and make more accurate predictions.

2. `Tournament Predictors`:
  - Tournament predictors run multiple branch prediction algorithms concurrently and select the most successful predictor for making predictions.
  - These predictors consist of multiple predictor components, such as local predictors, global predictors, or other types of predictors.
  - When a branch is encountered, each predictor component makes its prediction independently. A tournament mechanism then selects the prediction from the most successful component based on past accuracy.
  - Example:
    - A tournament predictor might include both a local history predictor and a global history predictor. Each predictor makes its prediction, and the tournament mechanism selects the prediction with the highest accuracy based on past performance.
    - Over time, if one predictor component consistently outperforms the others, it will be given more weight in the tournament selection process.
  - By running multiple predictors in parallel and selecting the most successful one, tournament predictors can adapt to different types of branch patterns and improve overall prediction accuracy.

In summary, advanced branch prediction techniques like correlating branch predictors and tournament predictors leverage sophisticated algorithms and historical branch behavior to make accurate predictions, thereby reducing pipeline stalls and improving processor performance.

**Refs**:
- [Correlating Branch Prediction](https://www.geeksforgeeks.org/correlating-branch-prediction/)

---

## Page 12

















